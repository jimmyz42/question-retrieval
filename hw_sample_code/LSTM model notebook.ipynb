{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import my_utils\n",
    "from my_utils import Tag, IobTag, parse, Word, write_test_output_file\n",
    "from collections import Counter\n",
    "import numpy\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from maxent_non_contextual_model import phi\n",
    "from maxent_contextual_model import get_iob_tags_l\n",
    "\n",
    "nltk.download(\"words\")\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, tagset_size, batch_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.tagset_size = tagset_size\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden(self.batch_size)\n",
    "\n",
    "    def init_hidden(self, minibatch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, minibatch_size, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, minibatch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence_inp):\n",
    "        if len(sentence_inp.size())==3:\n",
    "            num_sentences = self.batch_size\n",
    "        else:\n",
    "            num_sentences = 1\n",
    "        self.hidden = self.init_hidden(num_sentences)\n",
    "        sentence = sentence_inp.view((155, num_sentences, -1))\n",
    "        lstm_out, self.hidden = self.lstm(sentence, self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        tag_scores_out = tag_scores.view((num_sentences, 155, self.tagset_size))\n",
    "        return tag_scores_out\n",
    "\n",
    "nltk.download(\"words\")\n",
    "\n",
    "class TagDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features_mat, target_vector):\n",
    "        dataset = []\n",
    "        for i in range(len(target_vector)):\n",
    "            dataset.append(dict(x=torch.FloatTensor(features_mat[i]),\n",
    "                                y=torch.FloatTensor(target_vector[i])))\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "\n",
    "def run_epoch(data, is_training, model, optimizer, class_weights, batch_size):\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                              shuffle=True, drop_last=True)\n",
    "    losses = []\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total = 0.\n",
    "    right = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        x = Variable(batch[\"x\"])\n",
    "        y = Variable(batch[\"y\"])\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        out = out.view((-1, 4))\n",
    "        y = y.view((-1))\n",
    "        #print(y.long())\n",
    "        if class_weights is not None:\n",
    "            loss = torch.nn.NLLLoss(weight=class_weights, ignore_index=4)(out, y.long())\n",
    "        else:\n",
    "            loss = torch.nn.NLLLoss()(out, y.long())\n",
    "\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total = total + y.size(0)\n",
    "        right = right + (predicted == y.data.long()).sum()\n",
    "\n",
    "        if is_training:\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        losses.append(loss.cpu().data[0])\n",
    "    avg_loss = numpy.mean(losses)\n",
    "    avg_accuracy = right / total\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "\n",
    "def train_model(train_data, model, batch_size=200, num_epochs=50, lr=1.0, weight_decay=0, class_weights=None):\n",
    "    print(\"start train_model\")\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(\"epoch\", epoch)\n",
    "        train_loss, train_acc = run_epoch(train_data, True, model, optimizer, class_weights, batch_size)\n",
    "        print(\"train_loss\", train_loss)\n",
    "        print(\"train_acc\", train_acc)\n",
    "\n",
    "def predict_proba(model, input_vec):\n",
    "    proba = model(Variable(torch.FloatTensor(input_vec))).data\n",
    "\n",
    "def predict_iob_tags(test_features_mat, model):\n",
    "    prediction_vec = list()\n",
    "    for sentence_vec in test_features_mat:\n",
    "        output = model(Variable(torch.FloatTensor(sentence_vec)))\n",
    "        predicted = torch.max(output.data, 2)[1][0]\n",
    "        predicted = predicted.numpy()\n",
    "        predicted[predicted == 3] = 2\n",
    "        predicted_tags = [IobTag.get_tag(enc) for enc in predicted]\n",
    "        prediction_vec.append(predicted_tags)\n",
    "    return prediction_vec\n",
    "\n",
    "def get_token_strs(test_features_mat, test_words_l, model):\n",
    "    iob_tags_l = predict_iob_tags(test_features_mat, model)\n",
    "    prediction_vec = list()\n",
    "    current_gene_tag = Tag.gene1\n",
    "    for i, test_words in enumerate(test_words_l):\n",
    "        sequence = list()\n",
    "        for j, test_word in enumerate(test_words):\n",
    "            iob_tag = iob_tags_l[i][j]\n",
    "            if iob_tag == IobTag.o:\n",
    "                tag = Tag.tag\n",
    "            elif iob_tag == IobTag.b:\n",
    "                current_gene_tag = Tag.get_other_gene_tag(current_gene_tag)\n",
    "                tag = current_gene_tag\n",
    "            else:\n",
    "                tag = current_gene_tag\n",
    "            sequence.append(\"_\".join([test_word, tag]))\n",
    "        prediction_vec.append(\" \".join(sequence))\n",
    "    return prediction_vec\n",
    "\n",
    "def get_class_weights(train_target_vec):\n",
    "    train_target_vec_f = train_target_vec.flatten()\n",
    "    class_cnts = Counter(train_target_vec_f)  #list of encoded labels\n",
    "    class_weights = numpy.array(class_cnts.values(), dtype=numpy.float32)\n",
    "    class_weights = sum(class_weights) / class_weights # [10, 10, 1] = [GENE1, GENE2, TAG]\n",
    "    class_weights_tensor = torch.from_numpy(class_weights)\n",
    "    return class_weights_tensor\n",
    "\n",
    "def get_features_mat(words_l):\n",
    "    num_features = len(phi(\"testword\"))\n",
    "    features_mat = []\n",
    "    for words in words_l:\n",
    "        sentence_mat = numpy.zeros((155, num_features))\n",
    "        i = 0\n",
    "        for word in words:\n",
    "            sentence_mat[i] = phi(word)\n",
    "            i += 1\n",
    "            if i == 155:\n",
    "                break\n",
    "        features_mat.append(sentence_mat)\n",
    "    return numpy.array(features_mat)\n",
    "\n",
    "def get_target_vec(iob_tags_l):\n",
    "    target_vec = list()\n",
    "    for iob_tags in iob_tags_l:\n",
    "        sentence_tags = numpy.ones(155)*(4)\n",
    "        i = 0\n",
    "        for iob_tag in iob_tags:\n",
    "            sentence_tags[i] = IobTag.get_enc(iob_tag)\n",
    "            i +=1\n",
    "            if i == 155:\n",
    "                break\n",
    "        target_vec.append(sentence_tags)\n",
    "    return numpy.array(target_vec)\n",
    "\n",
    "def run(train_filename, test_filename, output_filename):\n",
    "    train_ids, train_words_l, train_tags_l = my_utils.parse(train_filename)\n",
    "    test_ids, test_words_l, ignore = my_utils.parse(test_filename)\n",
    "\n",
    "    train_iob_tags_l = get_iob_tags_l(train_tags_l)\n",
    "    train_features_mat = get_features_mat(train_words_l)\n",
    "    train_target_vec = get_target_vec(train_iob_tags_l)\n",
    "    train_dataset = TagDataset(train_features_mat, train_target_vec)\n",
    "\n",
    "    n_features = len(phi('testword'))\n",
    "    batch_size = 1000\n",
    "    model = LSTMTagger(n_features, 10, 4, batch_size)\n",
    "    lr = 1e-1\n",
    "    weight_decay = 1e-3\n",
    "    class_weights = get_class_weights(train_target_vec)\n",
    "    train_model(train_dataset, model, num_epochs=7, lr=lr,\n",
    "                weight_decay=weight_decay, class_weights=class_weights, batch_size=batch_size)\n",
    "\n",
    "    test_features_mat = get_features_mat(test_words_l)\n",
    "    test_token_strs = get_token_strs(test_features_mat, test_words_l, model)\n",
    "    my_utils.write_test_output_file(output_filename, test_ids, test_token_strs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('len(x)', 9000, 9000, 9000)\n",
      "('len(x)', 1500, 1500, 1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train_model\n",
      "('epoch', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:04<00:00,  1.93it/s]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_loss', 5.0256246460808649)\n",
      "('train_acc', 0.03684516129032258)\n",
      "('epoch', 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:04<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_loss', 4.9963762495252819)\n",
      "('train_acc', 0.03287240143369176)\n"
     ]
    }
   ],
   "source": [
    "train_filename = 'data/train.tag'\n",
    "output_filename = 'output4.tag'\n",
    "test_filename = 'data/dev.tag'\n",
    "train_ids, train_words_l, train_tags_l = my_utils.parse(train_filename)\n",
    "test_ids, test_words_l, ignore = my_utils.parse(test_filename)\n",
    "\n",
    "train_iob_tags_l = get_iob_tags_l(train_tags_l)\n",
    "train_features_mat = get_features_mat(train_words_l)\n",
    "train_target_vec = get_target_vec(train_iob_tags_l)\n",
    "train_dataset = TagDataset(train_features_mat, train_target_vec)\n",
    "\n",
    "n_features = len(phi('testword'))\n",
    "batch_size = 1000\n",
    "model = LSTMTagger(n_features, 10, 4, batch_size)\n",
    "lr = 1e-1\n",
    "weight_decay = 1e-3\n",
    "class_weights = get_class_weights(train_target_vec)\n",
    "train_model(train_dataset, model, num_epochs=2, lr=lr,\n",
    "                weight_decay=weight_decay, class_weights=class_weights, batch_size=batch_size)\n",
    "\n",
    "test_features_mat = get_features_mat(test_words_l)\n",
    "test_token_strs = get_token_strs(test_features_mat, test_words_l, model)\n",
    "my_utils.write_test_output_file(output_filename, test_ids, test_token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_tags_l = predict_iob_tags(test_features_mat, model)\n",
    "prediction_vec = list()\n",
    "current_gene_tag = Tag.gene1\n",
    "for i, test_words in enumerate(test_words_l):\n",
    "    sequence = list()\n",
    "    for j, test_word in enumerate(test_words):\n",
    "        iob_tag = iob_tags_l[i][j]\n",
    "        if iob_tag == IobTag.o:\n",
    "            tag = Tag.tag\n",
    "        elif iob_tag == IobTag.b:\n",
    "            current_gene_tag = Tag.get_other_gene_tag(current_gene_tag)\n",
    "            tag = current_gene_tag\n",
    "        else:\n",
    "            tag = current_gene_tag\n",
    "        sequence.append(\"_\".join([test_word, tag]))\n",
    "    prediction_vec.append(\" \".join(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_words in test_words_l:\n",
    "    if len(test_words) >  75:\n",
    "        print len(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
