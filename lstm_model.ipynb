{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tqdm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6d3214f482f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named tqdm"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, tagset_size, batch_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.tagset_size = tagset_size\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden(self.batch_size)\n",
    "\n",
    "    def init_hidden(self, minibatch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, minibatch_size, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, minibatch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence_inp):\n",
    "        if len(sentence_inp.size())==3:\n",
    "            num_sentences = self.batch_size\n",
    "        else:\n",
    "            num_sentences = 1\n",
    "        self.hidden = self.init_hidden(num_sentences)\n",
    "        sentence = sentence_inp.view((155, num_sentences, -1))\n",
    "        lstm_out, self.hidden = self.lstm(sentence, self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        tag_scores_out = tag_scores.view((num_sentences, 155, self.tagset_size))\n",
    "        return tag_scores_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataset, is_training, model, optimizer, batch_size):\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                              shuffle=True, drop_last=True)\n",
    "    losses = []\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total = 0.\n",
    "    right = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        q_body = Variable(batch[\"q_body\"]) # batch_size x 1 x truncate_length x 200]\n",
    "        p_body = Variable(batch[\"p_body\"])\n",
    "        neg_bodies = Variable(batch[\"neg_bodies\"]) # batch_size x num_negs x truncate_length x 200\n",
    "        q_title = Variable(batch[\"q_title\"])\n",
    "        p_title = Variable(batch[\"p_title\"])\n",
    "        neg_titles = Variable(batch[\"neg_titles\"])\n",
    "        num_negs = neg_titles.size()[1]\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "        q_body_enc, q_title_enc = model(q_body), model(q_title) # batch_size x 1 x enc_length\n",
    "        p_body_enc, p_title_enc = model(p_body), model(p_title)\n",
    "        neg_body_encs, neg_title_encs = model(neg_bodies), model(neg_titles) # batch_size x num_negs x enc_length\n",
    "        q_enc = q_title_enc + q_body_enc / 2.0\n",
    "        p_enc = p_title_enc + p_body_enc / 2.0\n",
    "        neg_encs = neg_title_encs + neg_body_encs / 2.0\n",
    "        candidate_encs = torch.cat((p_enc, neg_encs), dim=1) #batch_size x (num_negs + 1) x enc_length\n",
    "        query_encs = q_enc.repeat(1, num_negs, 1) # batch_size x (num_negs + 1) x enc_length\n",
    "        cos = torch.nn.CosineSimilarity(dim=2, eps=1e-08)(candidate_encs, query_encs) # batch_size x (num_negs + 1)\n",
    "        target = Variable(torch.zeros(batch_size).long(), requires_grad=True)\n",
    "        loss = torch.nn.MultiMarginLoss()(cos, target)\n",
    "\n",
    "        if is_training:\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        losses.append(loss.cpu().data[0])\n",
    "    avg_loss = numpy.mean(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, model, batch_size=200, num_epochs=50, lr=1.0, weight_decay=0):\n",
    "    print(\"start train_model\")\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(\"epoch\", epoch)\n",
    "        train_loss, train_acc = run_epoch(train_data, True, model, optimizer, batch_size)\n",
    "        print(\"train_loss\", train_loss)\n",
    "        print(\"train_acc\", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_input\n",
    "from read_input import QuestionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_TOKENIZED_FILE = 'askubuntu/text_tokenized.txt'\n",
    "TRAIN_FILE = 'askubuntu/train_random.txt'\n",
    "DEV_FILE = 'askubuntu/dev.txt'\n",
    "TEST_FILE = 'askubuntu/test.txt'\n",
    "\n",
    "TRUNCATE_LENGTH = 150\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "train_dataset = QuestionDataset(TEXT_TOKENIZED_FILE, TRAIN_FILE, truncate=TRUNCATE_LENGTH)\n",
    "\n",
    "train_model(train_dataset, model, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
