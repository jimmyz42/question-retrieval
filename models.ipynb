{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from evaluation import Evaluation\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embeddings, padding_idx, hidden_dim, num_layers, truncate_length, dropout=0.0, bidirectional=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        if bidirectional:\n",
    "            print('Bidirectinoal LSTM not implemented!!!')\n",
    "            assert(bidirectional==False)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.truncate_length= truncate_length\n",
    "        vocab_size, embed_dim = embeddings.shape\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.embedding_layer.weight.data = torch.from_numpy(embeddings)\n",
    "        self.embedding_layer.weight.requires_grad = False # Freezes the word vectors so we don't train them\n",
    "        # The LSTM takes word vectors as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        #self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)),\n",
    "                Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence_inp, mask):\n",
    "        # sentence_inp - batch_size x truncate_length\n",
    "        # mask - batch_size x truncate_length\n",
    "        batch_size = sentence_inp.size()[0]\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        sentence_vectorized = self.embedding_layer(sentence_inp).float()\n",
    "        # lstm expects batch_size x truncate_length x num_features because of batch_first=True\n",
    "        outputs_pre_dropout, self.hidden = self.lstm(sentence_vectorized)\n",
    "        outputs = self.dropout(outputs_pre_dropout)\n",
    "        out_masked = torch.mul(outputs, mask.unsqueeze(2).expand_as(outputs))\n",
    "        out_masked_avg = torch.div(out_masked.sum(dim=1), \n",
    "                                   mask.sum(dim=1).unsqueeze(1).expand(batch_size, self.hidden_dim))\n",
    "        return out_masked_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, truncate_length):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.truncate_length= truncate_length\n",
    "\n",
    "        self.conv = nn.Conv1d(input_dim, hidden_dim, 3, padding=1)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, sentence_inp):\n",
    "        # TODO modify this to take embedding indices\n",
    "        # sentence_inp is batch_size x truncate_length x num_features\n",
    "        outputs = self.conv(sentence_inp)\n",
    "        outputs = F.tanh(outputs)\n",
    "        outputs = self.drop(outputs)\n",
    "        out_masked = torch.mul(outputs, mask.unsqueeze(2).expand_as(outputs))\n",
    "        out_masked_avg = torch.div(out_masked.sum(dim=1), \n",
    "                                   mask.sum(dim=1).unsqueeze(1).expand(batch_size, self.hidden_dim))\n",
    "        return out_masked_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DomainClassifier(nn.Module):\n",
    "    # for us hidden_dim1 = 300, hidden_dim2 = 150\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(all_ranked_labels):\n",
    "    evaluator = Evaluation(all_ranked_labels)\n",
    "    MAP = evaluator.MAP()*100\n",
    "    MRR = evaluator.MRR()*100\n",
    "    P1 = evaluator.Precision(1)*100\n",
    "    P5 = evaluator.Precision(5)*100\n",
    "    return MAP, MRR, P1, P5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
