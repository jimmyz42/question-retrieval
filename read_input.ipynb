{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_tokenized(text_tokenized_file, truncate_length=100):\n",
    "    # returns a dictionary of {question_id : (title, body)} key-value pairs\n",
    "    question_id_to_title_body_tuple = {}\n",
    "    for line in open(text_tokenized_file, 'r'):\n",
    "        question_id, title, body = line.split('\\t')\n",
    "        question_id_to_title_body_tuple[question_id] = (title.split()[:truncate_length], \n",
    "                                                        body.split()[:truncate_length])\n",
    "    return question_id_to_title_body_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_ids(train_file):\n",
    "    # returns list of (question_id, positive_id, [negative_id, ...]) tuples\n",
    "    # where all ids are strings\n",
    "    train_id_instances = []\n",
    "    for line in open(train_file):\n",
    "        qid, positive_ids, negative_ids = line.split('\\t')\n",
    "        negative_ids = negative_ids.split()\n",
    "        for positive_id in positive_ids.split():\n",
    "            train_id_instances.append((qid, positive_id, negative_ids))\n",
    "    return train_id_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_to_vec_dict(word_embeddings_file):\n",
    "    word_to_vec = {}\n",
    "    for line in open(word_embeddings_file):\n",
    "        split_line = line.split()\n",
    "        word, vector = split_line[0], split_line[1:]\n",
    "        vector = np.array([float(x) for x in vector])\n",
    "        word_to_vec[word] = vector\n",
    "    return word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_file = 'askubuntu/vector/vectors_pruned.200.txt'\n",
    "word_to_vec = make_word_to_vec_dict(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_matrix_embedding(words, num_words=100):\n",
    "    # returns [num_words x length_embedding] np matrix\n",
    "    # matrix may be padded\n",
    "    if len(words) >  num_words:\n",
    "        # we shouldn't be printing here because we should have truncated already\n",
    "        print(len(words))\n",
    "    num_features = len(word_to_vec['.'])\n",
    "    sentence_mat = np.zeros((num_words, num_features))\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        # TODO: IS JUST SKIPPING THE WORD THE RIGHT APPROACH?\n",
    "        if word in word_to_vec:\n",
    "            sentence_mat[i] = word_to_vec[word]\n",
    "        i += 1\n",
    "        if i == num_words:\n",
    "            break\n",
    "    return sentence_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_tokenized_file, train_file, truncate=100):\n",
    "        self.truncate = truncate\n",
    "        self.id_to_question = read_text_tokenized(text_tokenized_file, truncate_length=self.truncate)\n",
    "        self.train_id_instances = read_train_ids(train_file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(train_id_instances)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        (q_id, positive_id, negative_ids) = self.train_id_instances[index]\n",
    "        q_title, q_body = self.id_to_question[q_id]\n",
    "        positive_title, positive_body = self.id_to_question[positive_id]\n",
    "        negative_title_body_tuples = [self.id_to_question[neg_id] for neg_id in negative_ids]\n",
    "        negative_bodies = [tup[1] for tup in negative_title_body_tuples]\n",
    "        q_body_matrix = Tensor(get_sentence_matrix_embedding(q_body, self.truncate))\n",
    "        positive_body_matrix = Tensor(get_sentence_matrix_embedding(positive_body, self.truncate))\n",
    "        negative_body_matrices = [(get_sentence_matrix_embedding(neg_body, self.truncate)) for \n",
    "                             neg_body in negative_bodies]\n",
    "        negative_body_matrices = Tensor(np.array(negative_body_matrices))\n",
    "        # negative_body_matrices is tensor of [100 x truncate_length x 200]\n",
    "        # q_body_matrix and positive_body_matrix are tensors of [truncate_length x 200]\n",
    "        return dict(q=q_body_matrix, p=positive_body_matrix, negatives=negative_body_matrices)\n",
    "\n",
    "dataset = QuestionDataset('askubuntu/text_tokenized.txt', 'askubuntu/train_random.txt', truncate=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0912 -0.1217 -0.0181  ...   0.1651 -0.0052  0.0591\n",
       " 0.0424 -0.0484  0.0034  ...   0.1301 -0.0354  0.0106\n",
       "-0.0387 -0.1000  0.0640  ...  -0.0286  0.0178  0.0194\n",
       "          ...             â‹±             ...          \n",
       " 0.1212 -0.1018  0.0021  ...   0.1048 -0.0324  0.0090\n",
       "-0.0264  0.0131 -0.0372  ...  -0.0599  0.0274  0.0208\n",
       "-0.0354 -0.0251  0.0562  ...  -0.0221  0.0141 -0.0239\n",
       "[torch.FloatTensor of size 150x200]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]['q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
