{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_tokenized(text_tokenized_file, truncate_length=100):\n",
    "    # returns a dictionary of {question_id : (title, body)} key-value pairs\n",
    "    question_id_to_title_body_tuple = {}\n",
    "    for line in open(text_tokenized_file, 'r'):\n",
    "        question_id, title, body = line.split('\\t')\n",
    "        question_id_to_title_body_tuple[question_id] = (title.split()[:truncate_length], \n",
    "                                                        body.split()[:truncate_length])\n",
    "    return question_id_to_title_body_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_ids(train_file, test_subset):\n",
    "    # returns list of (question_id, positive_id, [negative_id, ...]) tuples\n",
    "    # where all ids are strings\n",
    "    train_id_instances = []\n",
    "    i = 0\n",
    "    for line in open(train_file):\n",
    "        qid, positive_ids, negative_ids = line.split('\\t')\n",
    "        negative_ids = negative_ids.split()\n",
    "        for positive_id in positive_ids.split():\n",
    "            train_id_instances.append((qid, positive_id, negative_ids))\n",
    "            i += 1\n",
    "        if (test_subset is not None) and i > test_subset:\n",
    "            break\n",
    "    return train_id_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_to_vec_dict(word_embeddings_file):\n",
    "    word_to_vec = {}\n",
    "    for line in open(word_embeddings_file):\n",
    "        split_line = line.split()\n",
    "        word, vector = split_line[0], split_line[1:]\n",
    "        vector = np.array([float(x) for x in vector])\n",
    "        word_to_vec[word] = vector\n",
    "    return word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_file = 'askubuntu/vector/vectors_pruned.200.txt'\n",
    "word_to_vec = make_word_to_vec_dict(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_matrix_embedding(words, num_words=100):\n",
    "    # returns [num_words x length_embedding] np matrix\n",
    "    # matrix may be padded\n",
    "    if len(words) >  num_words:\n",
    "        # we shouldn't be printing here because we should have truncated already\n",
    "        print(len(words))\n",
    "    num_features = len(word_to_vec['.'])\n",
    "    sentence_mat = np.zeros((num_words, num_features))\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        # TODO: IS JUST SKIPPING THE WORD THE RIGHT APPROACH?\n",
    "        if word in word_to_vec:\n",
    "            sentence_mat[i] = word_to_vec[word]\n",
    "        i += 1\n",
    "        if i == num_words:\n",
    "            break\n",
    "    return sentence_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_tokenized_file, train_file, truncate=100, test_subset=None):\n",
    "        # test_subset: An integer representing the max number of training entries to consider.\n",
    "        #              Used for quick debugging on a smaller subset of all training data.\n",
    "        self.truncate = truncate\n",
    "        self.id_to_question = read_text_tokenized(text_tokenized_file, truncate_length=self.truncate)\n",
    "        self.train_id_instances = read_train_ids(train_file, test_subset)\n",
    "        self.num_features = len(word_to_vec['.'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_id_instances)\n",
    "    \n",
    "    def get_question_embedding(self, title_body_tuple):\n",
    "        title_embedding = Tensor(get_sentence_matrix_embedding(title_body_tuple[0], self.truncate))\n",
    "        body_embedding = Tensor(get_sentence_matrix_embedding(title_body_tuple[1], self.truncate))\n",
    "        return title_embedding, body_embedding\n",
    "    \n",
    "    def get_question_embeddings(self, title_body_tuples):\n",
    "        num_questions = len(title_body_tuples)\n",
    "        title_embeddings = np.zeros((num_questions, self.truncate, self.num_features))\n",
    "        body_embeddings = np.zeros((num_questions, self.truncate, self.num_features))\n",
    "        for i, (title, body) in enumerate(title_body_tuples):\n",
    "            title_embeddings[i] = get_sentence_matrix_embedding(title, self.truncate)\n",
    "            body_embeddings[i] = get_sentence_matrix_embedding(body, self.truncate)\n",
    "        return Tensor(title_embeddings), Tensor(body_embeddings)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        (q_id, positive_id, negative_ids) = self.train_id_instances[index]\n",
    "        q = self.id_to_question[q_id]\n",
    "        p = self.id_to_question[positive_id]\n",
    "        negative_ids_sample = random.sample(negative_ids, 20)  # sample 20 random negatives\n",
    "        negatives = [self.id_to_question[neg_id] for neg_id in negative_ids_sample]\n",
    "        q_title_embedding, q_body_embedding = self.get_question_embedding(q)\n",
    "        p_title_embedding, p_body_embedding = self.get_question_embedding(p)\n",
    "        neg_title_embeddings, neg_body_embeddings = self.get_question_embeddings(negatives)\n",
    "        # negative_body_matrices is tensor of [num_negs x truncate_length x 200]\n",
    "        # q_body_matrix and positive_body_matrix are tensors of [truncate_length x 200]\n",
    "        return dict(q_body=q_body_embedding, q_title=q_title_embedding, \n",
    "                    p_body=p_body_embedding, p_title=p_title_embedding, \n",
    "                    neg_bodies=neg_body_embeddings, neg_titles=neg_title_embeddings)\n",
    "\n",
    "#dataset = QuestionDataset('askubuntu/text_tokenized.txt', 'askubuntu/train_random.txt', truncate=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
