{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_tokenized(text_tokenized_file, truncate_length=100):\n",
    "    # returns a dictionary of {question_id : (title, body)} key-value pairs\n",
    "    question_id_to_title_body_tuple = {}\n",
    "    for line in open(text_tokenized_file, 'r'):\n",
    "        question_id, title, body = line.split('\\t')\n",
    "        question_id_to_title_body_tuple[question_id] = (title.split()[:truncate_length], \n",
    "                                                        body.split()[:truncate_length])\n",
    "    return question_id_to_title_body_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_ids(train_file):\n",
    "    # returns list of (question_id, positive_id, [negative_id, ...]) tuples\n",
    "    # where all ids are strings\n",
    "    train_id_instances = []\n",
    "    for line in open(train_file):\n",
    "        qid, positive_ids, negative_ids = line.split('\\t')\n",
    "        negative_ids = negative_ids.split()\n",
    "        for positive_id in positive_ids.split():\n",
    "            train_id_instances.append((qid, positive_id, negative_ids))\n",
    "    return train_id_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_to_vec_dict(word_embeddings_file):\n",
    "    word_to_vec = {}\n",
    "    for line in open(word_embeddings_file):\n",
    "        split_line = line.split()\n",
    "        word, vector = split_line[0], split_line[1:]\n",
    "        vector = np.array([float(x) for x in vector])\n",
    "        word_to_vec[word] = vector\n",
    "    return word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_file = 'askubuntu/vector/vectors_pruned.200.txt'\n",
    "word_to_vec = make_word_to_vec_dict(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_matrix_embedding(words, num_words=100):\n",
    "    # returns [num_words x length_embedding] np matrix\n",
    "    # matrix may be padded\n",
    "    if len(words) >  num_words:\n",
    "        # we shouldn't be printing here because we should have truncated already\n",
    "        print(len(words))\n",
    "    num_features = len(word_to_vec['.'])\n",
    "    sentence_mat = np.zeros((num_words, num_features))\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        # TODO: IS JUST SKIPPING THE WORD THE RIGHT APPROACH?\n",
    "        if word in word_to_vec:\n",
    "            sentence_mat[i] = word_to_vec[word]\n",
    "        i += 1\n",
    "        if i == num_words:\n",
    "            break\n",
    "    return sentence_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_tokenized_file, train_file, truncate=100):\n",
    "        # id_to_question is an optional pre-computed id_to_question\n",
    "        self.truncate = truncate\n",
    "        self.id_to_question = read_text_tokenized(text_tokenized_file, truncate_length=self.truncate)\n",
    "        self.train_id_instances = read_train_ids(train_file)\n",
    "        self.num_features = len(word_to_vec['.'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_id_instances)\n",
    "    \n",
    "#     def get_question_embedding(self, title_body_tuple):\n",
    "#         title_embedding = Tensor(get_sentence_matrix_embedding(title_body_tuple[0], self.truncate))\n",
    "#         body_embedding = Tensor(get_sentence_matrix_embedding(title_body_tuple[1], self.truncate))\n",
    "#         return title_embedding, body_embedding\n",
    "    \n",
    "    def get_question_embeddings(self, title_body_tuples):\n",
    "        num_questions = len(title_body_tuples)\n",
    "        title_embeddings = np.zeros((num_questions, self.truncate, self.num_features))\n",
    "        body_embeddings = np.zeros((num_questions, self.truncate, self.num_features))\n",
    "        for i, (title, body) in enumerate(title_body_tuples):\n",
    "            title_embeddings[i] = get_sentence_matrix_embedding(title, self.truncate)\n",
    "            body_embeddings[i] = get_sentence_matrix_embedding(body, self.truncate)\n",
    "        return Tensor(title_embeddings), Tensor(body_embeddings)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        (q_id, positive_id, negative_ids) = self.train_id_instances[index]\n",
    "        q = self.id_to_question[q_id]\n",
    "        p = self.id_to_question[positive_id]\n",
    "        negatives = [self.id_to_question[neg_id] for neg_id in negative_ids]\n",
    "        q_title_embedding, q_body_embedding = self.get_question_embeddings([q])\n",
    "        p_title_embedding, p_body_embedding = self.get_question_embeddings([p])\n",
    "        neg_title_embeddings, neg_body_embeddings = self.get_question_embeddings(negatives)\n",
    "        # negative_body_matrices is tensor of [num_negs x truncate_length x 200]\n",
    "        # q_body_matrix and positive_body_matrix are tensors of [1 x truncate_length x 200]\n",
    "        return dict(q_body=q_body_embedding, q_title=q_title_embedding, \n",
    "                    p_body=p_body_embedding, p_title=p_title_embedding, \n",
    "                    neg_bodies=neg_body_embeddings, neg_titles=neg_title_embeddings)\n",
    "\n",
    "#dataset = QuestionDataset('askubuntu/text_tokenized.txt', 'askubuntu/train_random.txt', truncate=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QuestionDataset('askubuntu/text_tokenized.txt', 'askubuntu/train_random.txt', truncate=150)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=13,\n",
    "                                              shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6292  0.4047  0.4759  ...   0.3253  0.3262  0.5561\n",
       " 0.4431  0.4948  0.4942  ...   0.3923  0.3356  0.3528\n",
       " 0.5343  0.4228  0.4100  ...   0.5150  0.4775  0.3825\n",
       "          ...             â‹±             ...          \n",
       " 0.5013  0.5764  0.4346  ...   0.3528  0.6759  0.4552\n",
       " 0.5808  0.5610  0.3754  ...   0.3485  0.5683  0.5332\n",
       " 0.5924  0.3875  0.5187  ...   0.6583  0.3167  0.4455\n",
       "[torch.FloatTensor of size 13x101]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_body = Variable(batch[\"q_body\"], requires_grad=True)\n",
    "p_body = Variable(batch[\"p_body\"], requires_grad=True)\n",
    "neg_bodies = Variable(batch[\"neg_bodies\"], requires_grad=True)\n",
    "q_title = Variable(batch[\"q_title\"], requires_grad=True)\n",
    "p_title = Variable(batch[\"p_title\"], requires_grad=True)\n",
    "neg_titles = Variable(batch[\"neg_titles\"], requires_grad=True)\n",
    "q_body_enc = q_body[:, :, :, 1] # batch_size x 1 x enc_length\n",
    "p_body_enc = p_body[:, :, :, 1]\n",
    "neg_body_encs = neg_bodies[:, :, :, 1] #batch_size x num_negs x enc_length\n",
    "q_title_enc = q_title[:, :, :, 1]\n",
    "p_title_enc = p_title[:, :, :, 1]\n",
    "neg_title_encs = neg_titles[:, :, :, 1]\n",
    "q_enc = q_title_enc + q_body_enc / 2.0\n",
    "p_enc = p_title_enc + p_body_enc / 2.0\n",
    "neg_encs = neg_title_encs + neg_body_encs / 2.0\n",
    "#p_enc = p_enc.resize(13, 1, 150)\n",
    "candidate_encs = torch.cat((p_enc, neg_encs), dim=1) #batch_size x (num_negs + 1) x enc_length\n",
    "#q_enc = q_enc.resize(13, 1, 150) \n",
    "query_encs = q_enc.repeat(1, 101, 1) # batch_size x (num_negs + 1) x enc_length\n",
    "cos = torch.nn.CosineSimilarity(dim=2, eps=1e-08)(candidate_encs, query_encs) # batch_size x (num_negs + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Variable(torch.zeros(13).long(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MultiMarginLoss()(cos, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101L"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_titles.size()[1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
