{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text_tokenized(text_tokenized_file, truncate_length=100):\n",
    "    # returns a dictionary of {question_id : (title, body)} key-value pairs\n",
    "    print('read corpus')\n",
    "    question_id_to_title_body_tuple = {}\n",
    "    for line in open(text_tokenized_file, 'r'):\n",
    "        question_id, title, body = line.split('\\t')\n",
    "        question_id_to_title_body_tuple[question_id] = (title.split()[:truncate_length], \n",
    "                                                        body.split()[:truncate_length])\n",
    "    return question_id_to_title_body_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_train_ids(train_file, test_subset):\n",
    "    # returns list of (question_id, positive_id, [negative_id, ...]) tuples\n",
    "    # where all ids are strings\n",
    "    train_id_instances = []\n",
    "    i = 0\n",
    "    for line in open(train_file):\n",
    "        qid, positive_ids, negative_ids = line.split('\\t')\n",
    "        negative_ids = negative_ids.split()\n",
    "        for positive_id in positive_ids.split():\n",
    "            train_id_instances.append((qid, positive_id, negative_ids))\n",
    "            i += 1\n",
    "        if (test_subset is not None) and i > test_subset:\n",
    "            break\n",
    "    return train_id_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_eval_ids(eval_file, test_subset):\n",
    "    # returns list of (question_id, candidate_ids, labels) tuples\n",
    "    # where all ids are strings, and labels is a list of binary positive/negative for each candidate in candidate_ids\n",
    "    eval_id_instances = []\n",
    "    i = 0\n",
    "    for line in open(eval_file):\n",
    "        qid, positive_ids, candidate_ids, bm25scores = line.split('\\t')\n",
    "        positive_ids_set = set(positive_ids.split())\n",
    "        candidate_ids = candidate_ids.split()\n",
    "        bm25scores = [float(score) for score in bm25scores.split()]\n",
    "        if len(positive_ids_set) == 0:\n",
    "            continue\n",
    "        labels = [1 if cid in positive_ids_set else 0 for cid in candidate_ids]\n",
    "        assert(sum(labels)==len(positive_ids_set))\n",
    "        eval_id_instances.append((qid, candidate_ids, labels, bm25scores))\n",
    "        i += 1\n",
    "        if (test_subset is not None) and i > test_subset:\n",
    "            break\n",
    "    return eval_id_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_word_embeddings(word_embeddings_file):\n",
    "    #word_to_vec = {}\n",
    "    word_to_idx = {}\n",
    "    embeddings = []\n",
    "    for i, line in enumerate(open(word_embeddings_file)):\n",
    "        split_line = line.split()\n",
    "        word, vector = split_line[0], split_line[1:]\n",
    "        vector = np.array([float(x) for x in vector])\n",
    "        #word_to_vec[word] = vector\n",
    "        word_to_idx[word] = i\n",
    "        embeddings.append(vector)\n",
    "    # add one for padding\n",
    "    embeddings.append(np.zeros(len(vector)))\n",
    "    padding_idx = i+1\n",
    "    embeddings = np.array(embeddings)\n",
    "    return word_to_idx, embeddings, padding_idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_matrix_embedding(words, num_words=100):\n",
    "    # returns [num_words] np matrix\n",
    "    # matrix may be padded\n",
    "    if len(words) >  num_words:\n",
    "        # we shouldn't be printing here because we should have truncated already\n",
    "        print(len(words))\n",
    "    sentence_mat = np.ones(num_words) * padding_idx\n",
    "    i = 0\n",
    "    mask = np.zeros(num_words)\n",
    "    for word in words:\n",
    "        # TODO: IS JUST SKIPPING THE WORD THE RIGHT APPROACH?\n",
    "        if word in word_to_idx:\n",
    "            sentence_mat[i] = word_to_idx[word]\n",
    "        mask[i] = 1\n",
    "        i += 1\n",
    "        if i == num_words:\n",
    "            break\n",
    "    return sentence_mat, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('reading word embedding file')\n",
    "word_embeddings_file = 'askubuntu/vector/vectors_pruned.200.txt'\n",
    "word_to_idx, embeddings, padding_idx = read_word_embeddings(word_embeddings_file)\n",
    "NUM_FEATURES = embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_tokenized, truncate):\n",
    "        # text_tokenized: Either a string representing the filename of text_tokenized, OR\n",
    "        #                 a precomputed id_to_question dictionary\n",
    "        self.truncate = truncate\n",
    "        if type(text_tokenized)==str:\n",
    "            self.id_to_question = read_text_tokenized(text_tokenized, truncate_length=self.truncate)\n",
    "        elif type(text_tokenized)==dict:\n",
    "            self.id_to_question = text_tokenized\n",
    "        #self.num_features = len(word_to_vec['.'])\n",
    "    \n",
    "    def get_question_embedding(self, title_body_tuple):\n",
    "        title_embedding, title_mask = get_sentence_matrix_embedding(title_body_tuple[0], self.truncate)\n",
    "        body_embedding, body_mask = get_sentence_matrix_embedding(title_body_tuple[1], self.truncate)\n",
    "        return Tensor(title_embedding), Tensor(body_embedding), Tensor(title_mask), Tensor(body_mask)\n",
    "    \n",
    "    def get_question_embeddings(self, title_body_tuples):\n",
    "        num_questions = len(title_body_tuples)\n",
    "        title_embeddings = np.zeros((num_questions, self.truncate))\n",
    "        body_embeddings = np.zeros((num_questions, self.truncate))\n",
    "        title_masks = np.zeros((num_questions, self.truncate))\n",
    "        body_masks = np.zeros((num_questions, self.truncate))\n",
    "        for i, (title, body) in enumerate(title_body_tuples):\n",
    "            title_embedding, title_mask = get_sentence_matrix_embedding(title, self.truncate)\n",
    "            body_embedding, body_mask = get_sentence_matrix_embedding(body, self.truncate)\n",
    "            title_embeddings[i] = title_embedding\n",
    "            body_embeddings[i] = body_embedding\n",
    "            title_masks[i] = title_mask\n",
    "            body_masks[i] = body_mask\n",
    "        return Tensor(title_embeddings), Tensor(body_embeddings), Tensor(title_masks), Tensor(body_masks)\n",
    "    \n",
    "    def get_q_candidate_dict(self, q_id, candidate_ids):\n",
    "        q = self.id_to_question[q_id]\n",
    "        candidates = [self.id_to_question[id_] for id_ in candidate_ids]\n",
    "        q_title_embedding, q_body_embedding, q_title_mask, q_body_mask = self.get_question_embedding(q)\n",
    "        (candidate_title_embeddings, candidate_body_embeddings, \n",
    "         candidate_title_masks, candidate_body_masks) = self.get_question_embeddings(candidates)\n",
    "        # candidate_*_embeddings is tensor of [num_cands x truncate_length]\n",
    "        # q_*_embedding is tensor of [truncate_length]\n",
    "        return dict(q_body=q_body_embedding.long(), q_title=q_title_embedding.long(),\n",
    "            candidate_bodies=candidate_body_embeddings.long(), candidate_titles=candidate_title_embeddings.long(), \n",
    "            q_body_mask = q_body_mask, q_title_mask=q_title_mask, \n",
    "            candidate_body_masks=candidate_body_masks, candidate_title_masks=candidate_title_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainQuestionDataset(QuestionDataset):\n",
    "    def __init__(self, text_tokenized_file, train_file, truncate=100, test_subset=None):\n",
    "        # test_subset: An integer representing the max number of training entries to consider.\n",
    "        #              Used for quick debugging on a smaller subset of all training data.\n",
    "        self.train_id_instances = read_train_ids(train_file, test_subset)\n",
    "        QuestionDataset.__init__(self, text_tokenized_file, truncate)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_id_instances)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        (q_id, positive_id, negative_ids) = self.train_id_instances[index]\n",
    "        negative_ids_sample = random.sample(negative_ids, 20)  # sample 20 random negatives\n",
    "        candidate_ids = [positive_id]+negative_ids_sample\n",
    "        return self.get_q_candidate_dict(q_id, candidate_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvalQuestionDataset(QuestionDataset):\n",
    "    def __init__(self, text_tokenized, eval_file, truncate=100, test_subset=None):\n",
    "        # test_subset: An integer representing the max number of entries to consider.\n",
    "        #              Used for quick debugging on a smaller subset of all eval data.\n",
    "        # text_tokenized: Either a string representing the filename of text_tokenized, OR\n",
    "        #                 a precomputed id_to_question dictionary\n",
    "        self.eval_id_instances = read_eval_ids(eval_file, test_subset)\n",
    "        QuestionDataset.__init__(self, text_tokenized, truncate)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.eval_id_instances)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        (q_id, candidate_ids, labels, bm25scores) = self.eval_id_instances[index]\n",
    "        item = self.get_q_candidate_dict(q_id, candidate_ids)\n",
    "        item['labels'] = Tensor(labels)\n",
    "        item['bm25scores'] = Tensor(bm25scores)\n",
    "        return item    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEXT_TOKENIZED_FILE = 'askubuntu/text_tokenized.txt'\n",
    "# TRAIN_FILE = 'askubuntu/train_random.txt'\n",
    "# DEV_FILE = 'askubuntu/dev.txt'\n",
    "# TEST_FILE = 'askubuntu/test.txt'\n",
    "\n",
    "# TRUNCATE_LENGTH = 150\n",
    "# train_dataset = TrainQuestionDataset(TEXT_TOKENIZED_FILE, TRAIN_FILE, truncate=TRUNCATE_LENGTH, test_subset=9)\n",
    "# id_to_question = train_dataset.id_to_question\n",
    "# dev_dataset = EvalQuestionDataset(train_dataset.id_to_question, DEV_FILE, truncate=TRUNCATE_LENGTH, test_subset=9)\n",
    "# test_dataset = EvalQuestionDataset(train_dataset.id_to_question, TEST_FILE, truncate=TRUNCATE_LENGTH, test_subset=9)\n",
    "# train_dataset[0]['q_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
