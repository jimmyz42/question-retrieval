{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word embedding file\n"
     ]
    }
   ],
   "source": [
    "import read_input\n",
    "from read_input import TrainQuestionDataset, EvalQuestionDataset, read_word_embeddings\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from evaluation import Evaluation\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from models import LSTM, CNN, evaluate, DomainClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasnan(var): \n",
    "    return np.isnan(np.sum(var.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataset, is_training, model, optimizer, batch_size, margin, save_path):\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                              shuffle=True, drop_last=True)\n",
    "    losses = []\n",
    "    all_ranked_labels = []\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    requires_grad = False\n",
    "    for batch in tqdm(data_loader):\n",
    "        q_body = Variable(batch[\"q_body\"], requires_grad=requires_grad) # batch_size x truncate_length\n",
    "        cand_bodies = Variable(batch[\"candidate_bodies\"], requires_grad=requires_grad) # batch_size x num_cands x truncate_length\n",
    "        q_title = Variable(batch[\"q_title\"], requires_grad=requires_grad)\n",
    "        cand_titles = Variable(batch[\"candidate_titles\"], requires_grad=requires_grad)\n",
    "        q_body_mask = Variable(batch[\"q_body_mask\"], requires_grad=requires_grad) # batch_size x truncate_length\n",
    "        q_title_mask = Variable(batch[\"q_title_mask\"], requires_grad=requires_grad)\n",
    "        cand_body_masks = Variable(batch[\"candidate_body_masks\"], requires_grad=requires_grad) # batch_size x num_cands x truncate_length\n",
    "        cand_title_masks = Variable(batch[\"candidate_title_masks\"], requires_grad=requires_grad)\n",
    "        num_cands = cand_titles.size()[1]\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "        q_body_enc, q_title_enc = model(q_body, q_body_mask), model(q_title, q_title_mask) # output is batch_size  x enc_length\n",
    "        cand_body_encs = model(cand_bodies.view(batch_size*num_cands, TRUNCATE_LENGTH), # output is (batch_size x num_cands) x enc_length\n",
    "                               cand_body_masks.view(batch_size*num_cands, TRUNCATE_LENGTH)) \n",
    "        cand_title_encs = model(cand_titles.view(batch_size*num_cands, TRUNCATE_LENGTH),\n",
    "                                cand_title_masks.view(batch_size*num_cands, TRUNCATE_LENGTH))\n",
    "        q_enc = q_title_enc + q_body_enc / 2.0\n",
    "        candidate_encs = cand_title_encs + cand_body_encs / 2.0\n",
    "        enc_length = q_enc.size()[-1]\n",
    "        #domain_predictions = domain_classifier(q_enc, candidate_ends)\n",
    "        #loss(domain_predictions, target_predictions)\n",
    "        #domain_optimizer.step()\n",
    "        candidate_encs = candidate_encs.view(batch_size, num_cands, -1) # batch_size x num_cands x enc_length\n",
    "        query_encs = q_enc.view(batch_size, 1, -1).expand_as(candidate_encs) # batch_size x (num_cands) x enc_length\n",
    "        cos = torch.nn.CosineSimilarity(dim=2, eps=1e-08)(candidate_encs, query_encs) # batch_size x (num_cands)\n",
    "        \n",
    "        if is_training:\n",
    "            target = Variable(torch.zeros(batch_size).long(), requires_grad=True)\n",
    "            loss = torch.nn.MultiMarginLoss(margin=margin)(cos, target)\n",
    "            #total_loss = loss - domain_loss\n",
    "            #total_loss.backward()\n",
    "            loss.backward(retain_graph=False)\n",
    "            optimizer.step()\n",
    "            losses.append(loss.cpu().data[0])\n",
    "        else:\n",
    "            # do evaluation stuff\n",
    "            sorted_cos, ind = cos.sort(1, descending=True)\n",
    "            labels = batch[\"labels\"]\n",
    "            for i in range(batch_size): \n",
    "                all_ranked_labels.append(labels[i][ind.data[i]])\n",
    "    if is_training:\n",
    "        # save the model\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        avg_loss = np.mean(losses)\n",
    "        return avg_loss\n",
    "    else:\n",
    "        return evaluate(all_ranked_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, dev_data, test_data, model, save_dir=None, batch_size=50, margin=1, num_epochs=50, lr=1.0, weight_decay=0):\n",
    "    if (save_dir is not None) and (not os.path.exists(save_dir)):\n",
    "        os.makedirs(save_dir)\n",
    "    print(\"start train_model\")\n",
    "    print(\"****************************************\")\n",
    "    print(\"Batch size: {}, margin: {}, num_epochs: {}, lr: {}\".format(batch_size, margin, num_epochs, lr))\n",
    "    print(\"Model\", model)\n",
    "    print(\"*****************************************\")\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    result_table = PrettyTable([\"Epoch\", \"train loss\", \"dev MAP\", \"dev MRR\", \"dev P@1\", \"dev P@5\"] +\n",
    "                                    [\"tst MAP\", \"tst MRR\", \"tst P@1\", \"tst P@5\"])\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(\"epoch\", epoch)\n",
    "        if save_dir is None:\n",
    "            save_path = None\n",
    "        else:\n",
    "            save_path = os.path.join(save_dir, 'epoch{}.pkl'.format(epoch))\n",
    "        train_loss = run_epoch(train_data, True, model, optimizer, batch_size, margin, save_path)\n",
    "        dev_MAP, dev_MRR, dev_P1, dev_P5 = run_epoch(dev_data, False, model, optimizer, batch_size, margin, save_path)\n",
    "        test_MAP, test_MRR, test_P1, test_P5 = run_epoch(test_data, False, model, optimizer, batch_size, margin, save_path)\n",
    "        result_table.add_row(\n",
    "                            [ epoch ] +\n",
    "                            [ \"%.3f\" % x for x in [train_loss] + [ dev_MAP, dev_MRR, dev_P1, dev_P5 ] +\n",
    "                                        [ test_MAP, test_MRR, test_P1, test_P5 ] ])\n",
    "        print(\"{}\".format(result_table))\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDINGS_FILE = 'askubuntu/vector/vectors_pruned.200.txt'\n",
    "\n",
    "TEXT_TOKENIZED_FILE = 'askubuntu/text_tokenized.txt'\n",
    "TRAIN_FILE = 'askubuntu/train_random.txt'\n",
    "DEV_FILE = 'askubuntu/dev.txt'\n",
    "TEST_FILE = 'askubuntu/test.txt'\n",
    "\n",
    "TRUNCATE_LENGTH = 100\n",
    "word_to_idx, embeddings, padding_idx = read_input.read_word_embeddings(WORD_EMBEDDINGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainQuestionDataset(TEXT_TOKENIZED_FILE, TRAIN_FILE, word_to_idx, padding_idx, truncate=TRUNCATE_LENGTH, test_subset=1000)\n",
    "dev_dataset = EvalQuestionDataset(train_dataset.id_to_question, DEV_FILE, word_to_idx, padding_idx, truncate=TRUNCATE_LENGTH, test_subset=1000)\n",
    "test_dataset = EvalQuestionDataset(train_dataset.id_to_question, TEST_FILE, word_to_idx, padding_idx, truncate=TRUNCATE_LENGTH, test_subset=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT_PROBS = [0.0, 0.1, 0.2, 0.3] # Taken from paper\n",
    "DROPOUT = 0.1\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "#model = LSTM(embeddings, padding_idx, 15, 1, TRUNCATE_LENGTH, DROPOUT, BIDIRECTIONAL)\n",
    "model = CNN(embeddings, padding_idx, 667, TRUNCATE_LENGTH, DROPOUT)\n",
    "# Example of how to load a previously trained model\n",
    "# model.load_state_dict(torch.load('lstm_saved_models/epoch1.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train_model\n",
      "****************************************\n",
      "Batch size: 20, margin: 0.2, num_epochs: 4, lr: 0.001\n",
      "('Model', CNN (\n",
      "  (embedding_layer): Embedding(100407, 200, padding_idx=100406)\n",
      "  (conv): Conv1d(200, 667, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (drop): Dropout (p = 0.1)\n",
      "))\n",
      "*****************************************\n",
      "('epoch', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [08:17<00:00,  9.95s/it]\n",
      "100%|██████████| 9/9 [00:32<00:00,  3.60s/it]\n",
      "100%|██████████| 9/9 [00:31<00:00,  3.50s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "| Epoch | train loss | dev MAP | dev MRR | dev P@1 | dev P@5 | tst MAP | tst MRR | tst P@1 | tst P@5 |\n",
      "+-------+------------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|   1   |    0.04    |  51.38  |  63.60  |  48.89  |  41.44  |  52.58  |  65.97  |  51.11  |  40.56  |\n",
      "+-------+------------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "('epoch', 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [08:47<00:00, 10.55s/it]\n",
      "100%|██████████| 9/9 [00:52<00:00,  5.89s/it]\n",
      "100%|██████████| 9/9 [00:52<00:00,  5.81s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "| Epoch | train loss | dev MAP | dev MRR | dev P@1 | dev P@5 | tst MAP | tst MRR | tst P@1 | tst P@5 |\n",
      "+-------+------------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|   1   |    0.04    |  51.38  |  63.60  |  48.89  |  41.44  |  52.58  |  65.97  |  51.11  |  40.56  |\n",
      "|   2   |    0.01    |  52.69  |  65.44  |  49.44  |  41.89  |  52.17  |  64.09  |  48.33  |  39.89  |\n",
      "+-------+------------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "('epoch', 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [07:11<03:42, 13.08s/it]"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 20 # Normally use 16\n",
    "NUM_EPOCHS = 4 # Normally use 50, but can stop early at 20\n",
    "MARGINS = [0.2, 0.4, 0.6] # Some student on piazza said 0.2 worked really well\n",
    "MARGIN = 0.2\n",
    "LRS = [1e-3, 3e-4] # Taken from paper\n",
    "LR = 1e-3\n",
    "\n",
    "#SAVE_DIR = 'lstm_saved_models'\n",
    "SAVE_DIR = 'cnn_saved_models'\n",
    "\n",
    "train_model(train_dataset, dev_dataset, test_dataset, model, SAVE_DIR,\n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            margin=MARGIN, batch_size=BATCH_SIZE, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
