{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_input\n",
    "from read_input import read_word_embeddings\n",
    "from prettytable import PrettyTable\n",
    "import random\n",
    "from evaluation import Evaluation\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from meter_auc import AUCMeter\n",
    "from models import LSTM, CNN, evaluate, DomainClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_tokenized(text_tokenized_file, truncate_length=100):\n",
    "    # returns a dictionary of {question_id : (title, body)} key-value pairs\n",
    "    print('read corpus', text_tokenized_file)\n",
    "    question_id_to_title_body_tuple = {}\n",
    "    all_questions_pieces = []\n",
    "    for line in open(text_tokenized_file, 'r'):\n",
    "        question_id, title, body = line.lower().split('\\t')\n",
    "        #title = title.split()[:truncate_length]\n",
    "        #body = body.split()[:truncate_length]\n",
    "        if len(title) == 0:\n",
    "            title = 'title'\n",
    "        if len(body) == 0:\n",
    "            body = 'body'\n",
    "        all_questions_pieces.append(title)\n",
    "        all_questions_pieces.append(body)\n",
    "        question_id_to_title_body_tuple[question_id] = (title, body)\n",
    "    return question_id_to_title_body_tuple, all_questions_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, model):\n",
    "    meter = AUCMeter()\n",
    "    for sample in tqdm(dataset):\n",
    "        q_body = sample[\"q_body\"] \n",
    "        cand_bodies = sample[\"candidate_bodies\"]\n",
    "        q_title = sample[\"q_title\"]\n",
    "        cand_titles = sample[\"candidate_titles\"]\n",
    "        num_cands = len(cand_titles)\n",
    "        q_body_enc = model.transform([q_body])\n",
    "        q_title_enc = model.transform([q_title])\n",
    "        cand_body_encs = model.transform(cand_bodies) \n",
    "        cand_title_encs = model.transform(cand_titles)\n",
    "        q_enc = q_title_enc + q_body_enc / 2.0\n",
    "        candidate_encs = cand_title_encs + cand_body_encs / 2.0\n",
    "\n",
    "        candidate_encs = candidate_encs.toarray()\n",
    "        query_encs = np.repeat(q_enc.toarray(), num_cands, axis=0)\n",
    "        sim = cosine_similarity(query_encs, candidate_encs, dense_output=True)\n",
    "        cos = sim[0]\n",
    "        labels = sample[\"labels\"]\n",
    "        meter.add(cos, labels)\n",
    "    return meter.value(), meter.value(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD_EMBEDDINGS_FILE = 'askubuntu/vector/vectors_pruned.200.txt'\n",
    "#WORD_EMBEDDINGS_FILE = 'vectors_stackexchange.txt'\n",
    "ANDROID_DEV_NEG_FILE = 'Android/dev.neg.txt'\n",
    "ANDROID_DEV_POS_FILE = 'Android/dev.pos.txt'\n",
    "ANDROID_TEST_NEG_FILE = 'Android/test.neg.txt'\n",
    "ANDROID_TEST_POS_FILE = 'Android/test.pos.txt'\n",
    "ANDROID_TEXT_TOKENIZED_FILE = 'Android/corpus.tsv'\n",
    "\n",
    "TRUNCATE_LENGTH = 100\n",
    "#word_to_idx, embeddings, padding_idx = read_word_embeddings(WORD_EMBEDDINGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AndroidEvalQuestionDataset(torch.utils.data.Dataset):\n",
    "    # Same idea as UbuntuEval, only difference is text_tokenized corpus file will be different, and\n",
    "    # no bm25scores. Not that we're using bm25 for UbuntuEval anyways.\n",
    "    def __init__(self, eval_pos_file, eval_neg_file, truncate=100, test_subset=None, num_negs=None):\n",
    "        self.eval_id_instances = read_input.read_android_eval_ids(eval_pos_file, eval_neg_file, test_subset)\n",
    "        self.num_negs = num_negs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.eval_id_instances)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        (q_id, candidate_ids, labels) = self.eval_id_instances[index]\n",
    "        pos_id = candidate_ids[0]\n",
    "        pos_label = labels[0]\n",
    "        if self.num_negs is None:\n",
    "            negative_ids = candidate_ids[1:]\n",
    "        else:\n",
    "            negative_ids = random.sample(candidate_ids[1:], self.num_negs)\n",
    "        candidate_ids = [pos_id] + negative_ids\n",
    "        candidate_labels = [1] + [0]*self.num_negs\n",
    "        query_title, query_body = question_id_to_questions[q_id]\n",
    "        candidate_titles = np.array([question_id_to_questions[i][0] for i in candidate_ids])\n",
    "        candidate_bodies = np.array([question_id_to_questions[i][1] for i in candidate_ids])\n",
    "\n",
    "        return dict(q_title=query_title, q_body=query_body, \n",
    "                    candidate_titles=candidate_titles, \n",
    "                    candidate_bodies=candidate_bodies, labels=np.array(candidate_labels))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For doing android eval alone (I.E. Part 2a - baselines)\n",
    "android_dev_dataset = AndroidEvalQuestionDataset(ANDROID_DEV_POS_FILE, ANDROID_DEV_NEG_FILE, \n",
    "                                                 truncate=100, test_subset=100, num_negs=20)\n",
    "android_test_dataset = AndroidEvalQuestionDataset(ANDROID_TEST_POS_FILE, ANDROID_TEST_NEG_FILE, \n",
    "                                                 truncate=100, test_subset=100, num_negs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_id_to_questions, all_questions_pieces = read_text_tokenized(ANDROID_TEXT_TOKENIZED_FILE, TRUNCATE_LENGTH)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(all_questions_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TFIDF BASELINE')\n",
    "dev_auc, dev_auc05 = evaluate_model(android_dev_dataset, vectorizer)\n",
    "print('dev auc {}, dev auc05 {}'.format(dev_auc, dev_auc05))\n",
    "test_auc, test_auc05 = evaluate_model(android_dev_dataset, vectorizer)\n",
    "print('dev auc {}, dev auc05 {}, test auc {}, test auc05 {}'.format(dev_auc, dev_auc05, test_auc, test_auc05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
